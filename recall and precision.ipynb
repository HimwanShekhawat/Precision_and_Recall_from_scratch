{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------\n",
    "# Load Fashion MNIST CSV\n",
    "# ---------------------------------\n",
    "df = pd.read_csv(\"/kaggle/input/fashion-mnist-train-csv/fashion-mnist_train.csv\")\n",
    "\n",
    "# Group by label to split each class separately\n",
    "grouped = df.groupby(\"label\")\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "# ---------------------------------\n",
    "# Split each class into train/test\n",
    "# ---------------------------------\n",
    "for label, group in grouped:\n",
    "    train_split, test_split = train_test_split(\n",
    "        group,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=None  # already grouped\n",
    "    )\n",
    "    train_list.append(train_split)\n",
    "    test_list.append(test_split)\n",
    "\n",
    "# Combine splits and shuffle\n",
    "train_df = pd.concat(train_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat(test_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# Plot one example per class\n",
    "# ---------------------------------\n",
    "examples = train_df.groupby(\"label\").first().reset_index()\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    img = examples.loc[i].drop(\"label\").values.astype(np.uint8).reshape(28, 28)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"Label: {examples.loc[i, 'label']}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------\n",
    "# Prepare training and test data\n",
    "# ---------------------------------\n",
    "X = train_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y = train_df[\"label\"].values\n",
    "num_classes = np.max(y) + 1\n",
    "y = np.eye(num_classes)[y]\n",
    "\n",
    "X_test = test_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y_test = test_df[\"label\"].values\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "# Normalize inputs\n",
    "np.random.seed(0)\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(X.shape, y.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# ---------------------------------\n",
    "# Activation functions\n",
    "# ---------------------------------\n",
    "def relu(x, grad):\n",
    "    if grad:\n",
    "        return (x > 0).astype(float)\n",
    "    else:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x, grad):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    if grad:\n",
    "        return s * (1 - s)\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def softmax(z, grad):\n",
    "    exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# Precision and Recall function\n",
    "# ---------------------------------\n",
    "def precision_recall(y_true, y_pred):\n",
    "    # Compute per-class true/false positives\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    return precision, recall\n",
    "\n",
    "# ---------------------------------\n",
    "# Network architecture\n",
    "# ---------------------------------\n",
    "arch = [300, 300, 10]\n",
    "activations = [relu, relu, softmax]\n",
    "\n",
    "W = []\n",
    "B = []\n",
    "\n",
    "alpha = 0.002\n",
    "batch = 32\n",
    "best_test = 0\n",
    "\n",
    "# He initialization\n",
    "for i in range(len(arch)):\n",
    "    if i == 0:\n",
    "        w = np.random.randn(X.shape[1], arch[i]) * np.sqrt(2. / X.shape[1])\n",
    "    else:\n",
    "        w = np.random.randn(arch[i-1], arch[i]) * np.sqrt(2. / arch[i-1])\n",
    "    b = np.zeros((1, arch[i]))\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "\n",
    "# ---------------------------------\n",
    "# Training loop\n",
    "# ---------------------------------\n",
    "whole_accuracy = []\n",
    "whole_accuracy_test = []\n",
    "whole_cost = []\n",
    "whole_cost_test = []\n",
    "\n",
    "for e in range(600):\n",
    "    all_accuracy = []\n",
    "    all_cost = []\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_accuracy_test = []\n",
    "    all_cost_test = []\n",
    "\n",
    "    # Mini-batch training\n",
    "    for i in range(int(np.ceil(len(X) / batch))):\n",
    "        X_batch = X[batch * i : batch * (i + 1)]\n",
    "        y_batch = y[batch * i : batch * (i + 1)]\n",
    "        m_batch = X_batch.shape[0]\n",
    "\n",
    "        # Forward pass\n",
    "        A = X_batch\n",
    "        all_A = []\n",
    "        all_Z = []\n",
    "        for j in range(len(W)):\n",
    "            Z = A @ W[j] + B[j]\n",
    "            A = activations[j](Z, grad=False)\n",
    "            all_A.append(A)\n",
    "            all_Z.append(Z)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        cost = (-1 / m_batch) * np.sum(y_batch * np.log(A + 1e-8))\n",
    "        all_cost.append(cost)\n",
    "\n",
    "        # Compute accuracy\n",
    "        y_pred = np.argmax(A, axis=1)\n",
    "        y_pred_oh = np.eye(num_classes)[y_pred]\n",
    "        y_true = np.argmax(y_batch, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_true) * 100\n",
    "        all_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute precision and recall\n",
    "        precision, recall = precision_recall(y_batch, y_pred_oh)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "        # Backpropagation\n",
    "        for j in reversed(range(len(W))):\n",
    "            if j == len(W) - 1:\n",
    "                dz = all_A[j] - y_batch\n",
    "            else:\n",
    "                dz = (dz @ W[j+1].T) * activations[j](all_Z[j], grad=True)\n",
    "\n",
    "            dw = X_batch.T @ dz if j == 0 else all_A[j-1].T @ dz\n",
    "            W[j] -= (alpha / m_batch) * dw\n",
    "            B[j] -= (alpha / m_batch) * np.sum(dz, axis=0, keepdims=True)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    for i_batch in range(int(np.ceil(len(X_test) / batch))):\n",
    "        X_test_batch = X_test[batch * i_batch : batch * (i_batch + 1)]\n",
    "        y_test_batch = y_test[batch * i_batch : batch * (i_batch + 1)]\n",
    "        m_test = X_test_batch.shape[0]\n",
    "\n",
    "        A = X_test_batch\n",
    "        for j in range(len(W)):\n",
    "            Z = A @ W[j] + B[j]\n",
    "            A = activations[j](Z, grad=False)\n",
    "\n",
    "        cost_test = (-1 / m_test) * np.sum(y_test_batch * np.log(A + 1e-8))\n",
    "        all_cost_test.append(cost_test)\n",
    "\n",
    "        y_pred_test = np.argmax(A, axis=1)\n",
    "        y_true_test = np.argmax(y_test_batch, axis=1)\n",
    "        accuracy_test = np.mean(y_pred_test == y_true_test) * 100\n",
    "        all_accuracy_test.append(accuracy_test)\n",
    "\n",
    "    # Log metrics\n",
    "    mean_acc_train = np.mean(all_accuracy)\n",
    "    mean_acc_test = np.mean(all_accuracy_test)\n",
    "    mean_cost_train = np.mean(all_cost)\n",
    "    mean_cost_test = np.mean(all_cost_test)\n",
    "    mean_precision = np.mean(all_precision)\n",
    "    mean_recall = np.mean(all_recall)\n",
    "\n",
    "    whole_accuracy.append(mean_acc_train)\n",
    "    whole_accuracy_test.append(mean_acc_test)\n",
    "    whole_cost.append(mean_cost_train)\n",
    "    whole_cost_test.append(mean_cost_test)\n",
    "\n",
    "    # Save best weights\n",
    "    if best_test < mean_acc_test:\n",
    "        best_test = mean_acc_test\n",
    "        for k in range(len(W)):\n",
    "            np.save(f\"/kaggle/working/W{k}.npy\", W[k])\n",
    "            np.save(f\"/kaggle/working/B{k}.npy\", B[k])\n",
    "\n",
    "    # Print progress\n",
    "    print(\n",
    "        f\"epochs: {e} | best_test {best_test:.2f} | \"\n",
    "        f\"accuracy_train: {mean_acc_train:.2f} | accuracy_test: {mean_acc_test:.2f} | \"\n",
    "        f\"cost_train: {mean_cost_train:.4f} | cost_test: {mean_cost_test:.4f} | \"\n",
    "        f\"precision: {mean_precision:.4f} | recall: {mean_recall:.4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
